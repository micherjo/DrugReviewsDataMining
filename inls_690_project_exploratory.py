# -*- coding: utf-8 -*-
"""INLS 690 - Project - Exploratory

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gI2llYBjKIv35HpgnO7UzHFL2bp8GrSE

##Drugs.com Reviews Exploratory Analysis

**Importing Libraries**
"""

# for basic operations
import numpy as np
import pandas as pd

# for basic visualizations
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('fivethirtyeight')

# for advanced visualizations
import plotly.offline as py
from plotly.offline import init_notebook_mode, iplot
import plotly.graph_objs as go
from plotly import tools
init_notebook_mode(connected = True)
import plotly.figure_factory as ff

# for providing the path
import os

"""**Reading the data**"""

data = pd.read_csv('drugsComTrain_raw.tsv', delimiter = '\t', quoting = 1, engine='python', encoding='utf-8', error_bad_lines=False)
# getting the shape of the data
data.shape

data.head()

# describing the data set

data.describe().round(2)

"""**Describing the data according to the Length of the reviews**"""

# adding a length column for analyzing the length of the reviews

data['length'] = data['review'].apply(len)

data.groupby('length').describe().sample(10)

"""**Describing the data according to the ratings**"""

data.groupby('rating').describe().round(2)

"""**Describing the data according to the drug Name**"""

data.groupby('drugName').describe().sample(10).round(2)

"""Describing data according to how many people found the review useful"""

data.groupby('usefulCount').describe().sample(10).round(2)

"""**Describe data according to condition treated**"""

data.groupby('condition').describe().sample(10).round(2)

"""## Data Visualizations"""

data['length'].value_counts().plot.hist(color = 'skyblue', figsize = (15, 5), bins = 50)
plt.title('Distribution of Length of Reviews')
plt.xlabel('Length of Reviews')
plt.ylabel('Number of reviews')
plt.show()

data[data['length'] == 21]['review'].iloc[0]

data[data['length'] == 50]['review'].iloc[0]

data[data['length'] == 150]['review'].iloc[0]

"""###Feature Extraction by Reviews"""

from sklearn.feature_extraction.text import TfidfVectorizer


cv = TfidfVectorizer(strip_accents=ascii, lowercase=True, analyzer='word', smooth_idf=True, stop_words = 'english')
words = cv.fit_transform(data.review)
sum_words = words.sum(axis=0)


words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]
words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)
frequency = pd.DataFrame(words_freq, columns=['word', 'freq'])

plt.style.use('fivethirtyeight')
color = plt.cm.ocean(np.linspace(0, 1, 20))
frequency.head(20).plot(x='word', y='freq', kind='bar', figsize=(15, 6), color = color)
plt.title("Most Frequently Occuring Words - Top 20")
plt.show()

from wordcloud import WordCloud

wordcloud = WordCloud(background_color = 'lightcyan', width = 2000, height = 2000).generate_from_frequencies(dict(words_freq))

plt.style.use('fivethirtyeight')
plt.figure(figsize=(10, 10))
plt.axis('off')
plt.imshow(wordcloud)
plt.title("Vocabulary from Reviews", fontsize = 20)
plt.show()

"""## Clean the text"""

# cleaning the text
# importing the libraries for Natural Language Processing

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

corpus = []

for i in range(0, 3150):
    review = re.sub('[^a-zA-Z]', ' ', data['review'][i])
    review = review.lower()
    review = review.split()
    ps = PorterStemmer()
    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]
    review = ' '.join(review)
    corpus.append(review)

# creating bag of words

from sklearn.feature_extraction.text import CountVectorizer


#vectorizer = TfidfVectorizer(stop_words = None).fit(train_corpus)

cv = CountVectorizer(stop_words = None).fit(data)
print(cv.vocabulary_)

from sklearn.model_selection import train_test_split


x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 15)


print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)